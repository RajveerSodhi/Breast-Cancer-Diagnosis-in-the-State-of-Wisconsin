{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yash Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1. **Which characteristic(s) most closely relate(s) to determining if a breast lump is cancerous or non-cancerous? How much weight does it carry in determining the diagnosis relative to the other features?**\n",
    "\n",
    "- I will analyze the correlation between each feature and the cancer diagnosis to understand their relationship better. To do this, I will create correlation heatmaps using Pearson correlation coefficients to visualize the correlations. After examining these correlations, I will select the features that show the highest correlations for further analysis. Finally, I will create detailed scatter plots, box plots, violin plots, and density plots using the 'seaborn' and 'matplotlib' libraries to understand better the relationship between the selected features and the diagnosis. Through this analysis, I will identify the characteristics with the highest correlation with a patient's cancer diagnosis. Furthermore, this analysis can help determine which features may be used as predictive cancer diagnosis markers and assist in making accurate diagnoses.\n",
    "\n",
    "***\n",
    "\n",
    "2. **How do the distributions of features vary across different sizes of tumours? More specifically, how does the importance of features vary with the size of the tumour?**\n",
    "\n",
    "- The dataset can be divided into three groups based on tumour size to explore how the distributions of features vary across different sizes of tumours. Then, visualizations such as histograms, density plots, and box plots can be used to compare the feature distributions across these groups. By examining these visualizations, one can identify whether certain features are more important for predicting malignancy or benignity in tumours of a particular size and whether the relationship between features and diagnosis varies across tumour sizes.\n",
    "\n",
    "***\n",
    "\n",
    "3. **How well can one predict whether a breast mass is cancerous or non-cancerous using the K-Nearest Neighbours or Support Vector Classifier models? (I am currently only familiar with KNN and SVC.)**\n",
    "\n",
    "- I have finished working on the third question and built a web application using Streamlit.\n",
    "\n",
    "- What have I done? \n",
    "    - To prepare the data, I employed 'LabelEncoder()' to transform the categorical data into numerical data and pipelines to streamline the ML process. In addition, 'StandardScalar()' was utilized to ensure all features were on the same scale. Subsequently, I employed SVC and KNN models to predict whether a patient was cancerous and 'accuracy_score()' to measure the model's accuracy. Finally, to obtain a more precise representation of the accuracy, I implemented 'cross_val_score().' Eventually, I settled on the Support Vector Classifier model, optimizing it with 'GridSearchCV(),' discovering the ideal parameters, and achieving a 'cross_val_score' of 97.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web App Link: [Streamlit App](https://kyash03-breastcancerwebapp-main-ckdp68.streamlit.app)**\n",
    "\n",
    "I have the code for the development of the ML model and the app itself in a folder called 'WebAppCode.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above output indicates that the values will need to be scaled. Scaling them will make it easier to compare variables and improve visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above cell's result shows that none of the columns have a missing value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We do not need the 'id' and 'Unnamed: 32' columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['id', 'Unnamed: 32'], axis=1).describe(\n",
    "#     include=np.number).apply(lambda x: x.apply(lambda y: format(y, 'f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above code removes the 'id' and 'Unnamed: 32' columns from the data frame and then applies the format() function to the output of the 'describe' method to display the results as floats.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['id', 'Unnamed: 32'], axis=1).describe(exclude=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following cell cleans the data, i.e., drops the unnecessary columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned = df.copy().drop(['id', 'Unnamed: 32'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above code creates a copy of the original frame to maintain an unmodified version of the dataset and then drops the unnecessary columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_output = pd.DataFrame()\n",
    "\n",
    "# for column_name in df_cleaned.columns[1:]:\n",
    "#     user_df = df_cleaned.groupby(\n",
    "#         by='diagnosis').describe().round(2).loc[:, column_name]\n",
    "#     user_df['feature_name'] = [column_name] * 2\n",
    "#     df_output = pd.concat([df_output, user_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_output.iloc[:, 1:][[\n",
    "#     'feature_name', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'\n",
    "# ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above output displays the statistics for each feature after being grouped by 'diagnosis.'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250)\n",
    "# fig_1 = sns.countplot(data=df_cleaned, x='diagnosis')\n",
    "# plt.xlabel('Diagnosis')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Diagnosis Counts')\n",
    "# fig_1.bar_label(fig_1.containers[0], label_type='center', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above graph tells us that the dataset contains a much larger number of benign samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = df_cleaned.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250, figsize=(20, 20))\n",
    "# fig_2 = sns.heatmap(corr,\n",
    "#                     annot=True,\n",
    "#                     mask=np.triu(np.ones_like(corr)),\n",
    "#                     fmt=\".2f\")\n",
    "# plt.title('Heatmap of Correlations between Columns', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250, figsize=(20, 20))\n",
    "\n",
    "# corr[abs(corr < 0.75)] = 0\n",
    "# fig_2 = sns.heatmap(corr,\n",
    "#                     annot=True,\n",
    "#                     mask=np.triu(np.ones_like(corr)),\n",
    "#                     fmt=\".2f\")\n",
    "\n",
    "# plt.title('Heatmap of Correlations between Columns', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The heatmap indicates that multiple variables have a high correlation, i.e., a correlation value greater than or equal to 0.75. Other than the apparent high correlation between the 'perimeter' and 'area' variables, what stands out is the strong relationship between the 'compactness,' 'concavity,' and 'concave points' variables.**\n",
    "\n",
    "**I will explore this relationship using regression plots later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x for x in df_cleaned.columns if '_mean' in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The histograms below indicate that most of the variables are right-skewed. The only variable that closely resembles a normal distribution is 'symmetry_mean.'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250, figsize=(16, 56))\n",
    "# for i, column_name in enumerate(df_cleaned.columns[1:]):\n",
    "#     fig = plt.subplot(10, 3, i + 1)\n",
    "#     fig.set_title('Distribution of ' + column_name)\n",
    "#     sns.histplot(data=df_cleaned[column_name], bins=50, color='grey', kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will standardize the data to get accurate values and visually informative graphs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scaler.fit(df_cleaned.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_data = pd.DataFrame(standard_scaler.transform(\n",
    "#     df_cleaned.iloc[:, 1:]),\n",
    "#                                  columns=df_cleaned.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250, figsize=(14, 8))\n",
    "# feature_names = ['perimeter', 'area']\n",
    "# for i, feature_name in enumerate(feature_names):\n",
    "#     plt.subplot(1, 2, i + 1)\n",
    "#     plt.title('Boxplot of ' + str.capitalize(feature_name[0]) +\n",
    "#               feature_name[1:],\n",
    "#               size=20)\n",
    "#     plt.xlabel('Feature')\n",
    "#     plt.ylabel('Standardized Value')\n",
    "#     sns.boxplot(data=standardized_data[\n",
    "#         [x for x in standardized_data.columns if feature_name in x]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "1. **The box plots indicate that the 'mean' values are more spread out than their counterparts, i.e., 'se' and 'worst. 'mean' values seem to have a large variation.**\n",
    "2. **The 'perimeter' and 'area' values are quite spread out, indicating that they're both most likely not centred around a particular value; instead, they take on a large range of values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250, figsize=(14, 8))\n",
    "# feature_names = ['perimeter', 'area']\n",
    "# for i, feature_name in enumerate(feature_names):\n",
    "#     plt.subplot(1, 2, i + 1)\n",
    "#     plt.title('Boxplot of ' + str.capitalize(feature_name[0]) +\n",
    "#               feature_name[1:] + ' (Without Outliers)',\n",
    "#               size=18)\n",
    "#     plt.xlabel('Feature')\n",
    "#     plt.ylabel('Standardized Value')\n",
    "#     sns.boxplot(data=standardized_data[[\n",
    "#         x for x in standardized_data.columns if feature_name in x\n",
    "#     ]],\n",
    "#                 showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By setting 'showfliers' to 'False,' outliers from the box plot have been removed, resulting in a more focused and accurate graph that avoids any potential misleading interpretations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250, figsize=(20, 6))\n",
    "# fig_3 = sns.violinplot(data=standardized_data.iloc[:, :10].assign(\n",
    "#     diagnosis=df_cleaned['diagnosis']).melt(id_vars='diagnosis'),\n",
    "#                        x='variable',\n",
    "#                        y='standardized value',\n",
    "#                        hue='diagnosis')\n",
    "# plt.title('Violin Plots of Mean Values', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250, figsize=(20, 6))\n",
    "# fig_4 = sns.violinplot(data=standardized_data.iloc[:, 10:20].assign(\n",
    "#     diagnosis=df_cleaned['diagnosis']).melt(id_vars='diagnosis'),\n",
    "#                        x='variable',\n",
    "#                        y='standardized value',\n",
    "#                        hue='diagnosis')\n",
    "# plt.title('Violin Plots of Standard Error Values', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=250, figsize=(20, 6))\n",
    "# fig_5 = sns.violinplot(data=standardized_data.iloc[:, 20:].assign(\n",
    "#     diagnosis=df_cleaned['diagnosis']).melt(id_vars='diagnosis'),\n",
    "#                        x='variable',\n",
    "#                        y='standardized value',\n",
    "#                        hue='diagnosis')\n",
    "# plt.title('Violin Plots of Worst Values', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As indicated previously, the values of 'perimeter' and 'area' vary greatly. However, it is important to note that this is only true for malignant diagnoses. Most benign cases are centred around a particular value.**\n",
    "\n",
    "**The above theory seems to hold regardless of the feature in question; for all features, the malignant diagnoses seem to take on a larger range of values compared to the benign diagnoses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_pairs = [('concavity_mean', 'concave points_mean', 'mean'),\n",
    "#                 ('concavity_se', 'concave points_se', 'se'),\n",
    "#                 ('concavity_worst', 'concave points_worst', 'worst')]\n",
    "\n",
    "# plt.figure(figsize=(20, 6), dpi=250)\n",
    "\n",
    "# for i, (x_col, y_col, title_suffix) in enumerate(column_pairs):\n",
    "#     plt.subplot(1, 3, i + 1)\n",
    "#     fig = sns.regplot(x=x_col, y=y_col, data=standardized_data)\n",
    "#     fig.set_title(\n",
    "#         f'Regression Plot of \"{x_col}\" vs \"{y_col}\" ({title_suffix})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "1. **Compared to the 'mean' and 'worst' values, the 'se' values seem centred around the same value, except for a couple of outliers.**\n",
    "2. **'concave points' and 'concavity' seem highly correlated, regardless of the statistic used. The heatmap previously made confirms this relationship.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'project_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [284]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mproject_functions\u001b[39;00m \u001b[38;5;66;03m# This is called a relative import\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m project_functions\u001b[38;5;241m.\u001b[39mload_and_process(url_or_path_to_csv_file)\n\u001b[1;32m      3\u001b[0m df\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'project_functions'"
     ]
    }
   ],
   "source": [
    "import project_functions3\n",
    "df = project_functions.load_and_process(url_or_path_to_csv_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
